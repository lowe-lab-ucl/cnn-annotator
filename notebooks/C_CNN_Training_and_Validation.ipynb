{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training for Cell Cycle State Classification\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This notebook allows you to train a convolutional neural network (CNN) using your annotated single-cell image patches & make new predictions about the labels of previously unseen images. Follow the step-wise instructions to proceed with the network training and testing of the accuracy. \n",
    "\n",
    "This is a preview of the CNN training process with image annotations: <br/>\n",
    "![image](../assets/cnn_training_process.png)\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. You are using the virtual environment of the [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb \"Google Colaboratory\"). To be able to train the neural network on your annotated data, you must first **import your data** into the folders to source from. Please follow the instructions after executing the first cell of this notebook.\n",
    "\n",
    "2. If using Google Colab: This session will 'timeout' if you do not interact with it. It's 90 minutes if you close the browser or 12 hours if you keep the browser open. Additionally, if you close your browser with a code cell is running, if that same cell has not finished, when you reopen the browser it will still be running (the current executing cell keeps running even after browser is closed). Please visit this [StackOverflow](https://stackoverflow.com/questions/54057011/google-colab-session-timeout \"Google Colab Session Timeout\") discussion for more details.\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. Prior to running the whole notebook in one go, make sure to execute the first cell containing code. This allows to install the CellX library & create local directories in the environment of the virtual machine. \n",
    "\n",
    "2. The executed first cell will print ```Building wheel for cellx (setup.py) ... done```. Click on the ``` ðŸ“``` folder icon located on the left-side dashboard of the Colab notebook. You should now see 4 subfolders in this directory: \"sample_data\" (default), \"logs\" \"train\" and \"test\" folder, which should all be empty.\n",
    "\n",
    "3. At this point, you should **manually move your 'annotation_XXX.zip' files into the \"train\" and \"test\" folders**. Doing so will allow the image patch data to be processed, divided into categories and used for model training & predictions.\n",
    "\n",
    "> When training your network, it is important you allocate some of your annotated data so that the images and their labels are not seen by the network in the training step. Doing so will enable you to test how well the model is able to predict new labels in new, previously unseen images. We recommend you distribute your annotation files so that there is **approx. 80% of the labels in the train group and the remainder 20% is used for testing stage**. If you fail to allocate enough examples for the testing dataset, you will be unable to validate the performance of the network. \n",
    "\n",
    "4. You can now now run the entire notebook by clicking on ```Runtime``` > ```Run``` in the upper main dashboard. Re-running the initial cell will fail to create the \"logs\" \"train\" and \"test\" folders as those are already in the directory. \n",
    "\n",
    "5. Prior to training of the model, this notebook will distribute the image patch data into the training & testing sets and introduce data augmentations. The notebook will ultimately train the neural network based on the hyperparameters you've set up.\n",
    "\n",
    "6. During training, you can actively visualise what the network is doing via [TensorBoard](https://www.tensorflow.org/tensorboard/get_started \"TensorFlow || Tensorboard\"), a tool for providing the measurements and visualizations needed during the machine learning workflow. It enables tracking experiment metrics like loss and accuracy, visualizing the model graph, projecting embeddings to a lower dimensional space, and much more.\n",
    "\n",
    "7. **Do not terminate this notebook before saving out the model.** To export and download the saved model to your local machine, press the '...' button and select 'Download'. Failure to save and download the model will result in losing all of the training progress you've achieved so far. You'll be instructed to import the downloaded model into the Colab environment again in the next steps of this protocol - make sure you have a working model to show for it. \n",
    "\n",
    "---\n",
    "\n",
    "**Happy training!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the CellX library & create subdirectories in the virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "    !mkdir logs\n",
    "    !mkdir train\n",
    "    !mkdir test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and CellX toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.dataset import build_dataset\n",
    "from cellx.tools.dataset import write_dataset\n",
    "from cellx.augmentation.utils import append_conditional_augmentation, augmentation_label_handler\n",
    "from cellx.callbacks import tensorboard_confusion_matrix_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths & class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"./train\"\n",
    "TEST_PATH = \"./test\"\n",
    "TRAIN_FILE = os.path.join(TRAIN_PATH, 'CNN_train.tfrecord')\n",
    "TEST_FILE = os.path.join(TEST_PATH, 'CNN_test.tfrecord')\n",
    "LABELS = [\"Interphase\", \"Prometaphase\", \"Metaphase\", \"Anaphase\", \"Apoptosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up CNN training hyperparameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20_000\n",
    "TRAINING_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Tensorboard extension for real-time visualisation of CNN training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "LOG_ROOT = './logs'\n",
    "LOG_DIR = os.path.join(LOG_ROOT, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate TensorFlow Record (TFRecord) files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(\n",
    "    root, \n",
    "    filename,\n",
    "    labels=LABELS\n",
    "):\n",
    "    \n",
    "    _images = []\n",
    "    _labels = []\n",
    "    \n",
    "    # Find the zip files:\n",
    "    zipfiles = [os.path.join(root, f) for f in os.listdir(root) if f.endswith(\".zip\") and f.startswith(\"annotation_\")]\n",
    "    \n",
    "    if len(zipfiles) == 0:\n",
    "        raise Exception(\"No 'annotation' zip files found in the directory. Please provide your annotated data into the Colab environment.\")\n",
    "    \n",
    "    label_counter = dict({\"Interphase\" : 0, \"Prometaphase\" : 0, \"Metaphase\" : 0, \"Anaphase\" : 0, \"Apoptosis\" : 0, \"Flagged\" : 0})\n",
    "    \n",
    "    for zfn in zipfiles:\n",
    "        print(f\"Loading file: {zfn}\")\n",
    "        with zipfile.ZipFile(zfn, 'r') as zip_data:\n",
    "            files = zip_data.namelist()\n",
    "\n",
    "            for numeric_label, label in enumerate(labels):\n",
    "                patch_files = [f for f in files if f.endswith(\".tif\") and f.startswith(label.capitalize())]\n",
    "                \n",
    "                # Count the label instances: \n",
    "                for f in patch_files:\n",
    "                    if f.endswith(\"_flagged.tif\"):\n",
    "                        label_counter[\"Flagged\"] += 1\n",
    "                    else:\n",
    "                        if f.startswith(label.capitalize()):\n",
    "                            label_counter[label.capitalize()] += 1\n",
    "                \n",
    "                # Open image patched & read the pixel data:\n",
    "                images = [plt.imread(zip_data.open(f)) for f in patch_files if not \"_flagged\" in f]\n",
    "                images_resized = [resize(img, (64, 64), preserve_range=True) for img in images]\n",
    "\n",
    "                _images += images_resized\n",
    "                _labels += [numeric_label] * len(images_resized)\n",
    "\n",
    "                \n",
    "    images_arr = np.stack(_images, axis=0)[..., np.newaxis]\n",
    "    labels_arr = np.stack(_labels, axis=0)\n",
    "    \n",
    "    # Print out the statistics:\n",
    "    print(f\"Total images: {images_arr.shape[0]}\")\n",
    "    print(label_counter)\n",
    "    \n",
    "    # Visualise the class distribution:\n",
    "    plt.bar(x=label_counter.keys(), height=label_counter.values(), color=\"grey\")\n",
    "    plt.title(f\"Label Count per Class: {root} set of {images_arr.shape[0]} images\")\n",
    "    plt.xticks(ticks=label_counter.keys(), labels=label_counter.keys(), rotation=45)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    write_dataset(filename, images_arr.astype(np.uint8), labels=labels_arr.astype(np.int64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: \n",
    "\n",
    "**Prior to calling the function to create the TFRecods files:**\n",
    "\n",
    "You need to manually drag the `annotation_XXX.zip` files into the newly created folders. If you are working in the Google Colab environment, click on the folder icon at the left-side dashboard, which should now contain the 'logs', 'train' and 'test' directories. They should be empty until you drag your annotation files into them.\n",
    "\n",
    "When training your network, it is important you reserve some of your annotated data so that it's not seen by the network in the training step - this is the most ideal way to test how well the model is able to predicts the new labels in completely new, previously unseen images. We recommend you distribute your annotation files so that there is **approx. 80% of the labels in the train group and the remainder 20% is used for testing stage**. If you fail to allocate enough examples for the testing dataset, you will be unable to validate the performance of the network. \n",
    "\n",
    "Once the files have been imported, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tf_record(TRAIN_PATH, TRAIN_FILE)\n",
    "create_tf_record(TEST_PATH, TEST_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a simple CNN for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = K.layers.Input(shape=(64, 64, 1))\n",
    "x = Encoder2D(layers=[8, 16, 32, 64, 128])(img)\n",
    "x = K.layers.Flatten()(x)\n",
    "x = K.layers.Dense(256, activation=\"relu\")(x)\n",
    "x = K.layers.Dropout(0.2)(x)\n",
    "logits = K.layers.Dense(5, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = K.Model(inputs=img, outputs=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up some augmentations to be used while training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def normalize(img):\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    # clip to 4 standard deviations\n",
    "    img = tf.clip_by_value(img, -4., 4.)\n",
    "    tf.debugging.check_numerics(img, \"Image contains NaN\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def augment(img):\n",
    "    boundary_augmentation=True\n",
    "    if boundary_augmentation:\n",
    "        # this will randomly simulate the cropping that occurs at the edge of\n",
    "        # an image volume\n",
    "\n",
    "        vignette = np.ones((64, 64, 1), dtype=np.float32)\n",
    "        width = np.random.randint(0,30)\n",
    "        vignette[:,:width,...] = 0\n",
    "\n",
    "        img = tf.cond(pred=tf.random.uniform(shape=())<0.05,\n",
    "                true_fn=lambda: tf.multiply(img, vignette),\n",
    "                false_fn=lambda: img)\n",
    "\n",
    "    # do some data augmentation\n",
    "    k = tf.random.uniform(maxval=3, shape=(), dtype=tf.int32)\n",
    "    img = tf.image.rot90(img, k=k)\n",
    "\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    img = tf.image.random_flip_up_down(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@augmentation_label_handler\n",
    "def random_contrast(x):\n",
    "    return tf.image.random_contrast(x, 0.3, 1.0)\n",
    "\n",
    "@augmentation_label_handler\n",
    "def random_brightness(x):\n",
    "    return tf.image.random_brightness(x, 0.3, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the training dataset, with random augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = build_dataset(TRAIN_FILE, read_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(augment)\n",
    "dataset = append_conditional_augmentation(dataset, [random_contrast, random_brightness])\n",
    "dataset = dataset.map(normalize)\n",
    "dataset = dataset.shuffle(buffer_size=BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "dataset = dataset.repeat()\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the test dataset, without augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = build_dataset(TEST_FILE, read_label=True)\n",
    "test_dataset = test_dataset.map(normalize)\n",
    "test_dataset = test_dataset.take(-1).as_numpy_iterator()\n",
    "\n",
    "test_images, test_labels = zip(*list(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up tensorboard callbacks to monitor training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callback = K.callbacks.TensorBoard(log_dir=LOG_DIR)\n",
    "confusion_matrix_callback = tensorboard_confusion_matrix_callback(\n",
    "    model, \n",
    "    np.asarray(test_images), \n",
    "    test_labels,\n",
    "    LOG_DIR,\n",
    "    class_names=LABELS,\n",
    "    is_binary=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = K.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=\"adam\", loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, train the model and evaluate performance using TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $LOG_ROOT --host localhost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    dataset, \n",
    "    steps_per_epoch=BUFFER_SIZE//BATCH_SIZE, \n",
    "    epochs=TRAINING_EPOCHS, \n",
    "    callbacks=[tensorboard_callback, confusion_matrix_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'\n",
    "model.save('{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT:\n",
    "\n",
    "Do not terminate this notebook before saving the model. To export and download the saved model to your local machine, press the '...' button and select 'Download'. Failure to save and download the model will result in losing all of the training progress you've achieved so far. You'll be instructed to import the downloaded model into the Colab environment again in the next steps of this protocol - make sure you have a working model to show for it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
