{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Testing for Cell Cycle State Classification\n",
    "\n",
    "### Welcome!\n",
    "\n",
    "This notebook allows you to take the convolutional neural network (CNN) that you trained in the previous notebook and evaluate its performance on a set of previously unseen single-cell image patches. Follow the step-wise instructions to proceed with testing the network.\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "1. You are using the virtual environment of the [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb \"Google Colaboratory\"). To be able to test the neural network, you must first **import images not used during training** into the folders to source from. Please follow the instructions after executing the first cell of this notebook.\n",
    "\n",
    "2. If using Google Colab: This session will 'timeout' if you do not interact with it. It's 90 minutes if you close the browser or 12 hours if you keep the browser open. Additionally, if you close your browser with a code cell is running, if that same cell has not finished, when you reopen the browser it will still be running (the current executing cell keeps running even after browser is closed). Please visit this [StackOverflow](https://stackoverflow.com/questions/54057011/google-colab-session-timeout \"Google Colab Session Timeout\") discussion for more details.\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. Execute the first cell containing code below, which will install the CellX library & create a local test directory in the environment of the virtual machine. The executed first cell will print ```Building wheel for cellx (setup.py) ... done```. (Note: This virtual environment is different from the one created for the Training notebook, which is why we need to re-install libraries etc.)\n",
    "\n",
    "2. Click on the ``` ðŸ“``` folder icon located on the left-side dashboard of the Colab notebook. Drag your annotated zip file(s) into the \"test\" folder. Drag your saved model (the `.h5` file) into the \"content\" folder - the parent folder of the \"test\" folder.\n",
    "\n",
    "3. You can now now run the entire notebook by clicking on ```Runtime``` > ```Run``` in the upper main dashboard. \n",
    "\n",
    "---\n",
    "\n",
    "**Happy testing!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the CellX library & create subdirectories in the virtual machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using colab, install cellx library and make log and data folders\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install -q git+git://github.com/quantumjot/cellx.git\n",
    "    !mkdir test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and CellX toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.special import softmax\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from umap import UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from cellx.core import load_model\n",
    "from cellx.layers import Encoder2D\n",
    "from cellx.tools.confusion import plot_confusion_matrix\n",
    "from cellx.tools.io import read_annotations\n",
    "from cellx.tools.projection import ManifoldProjection2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define paths & class labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PATH = \"./test\"\n",
    "LABELS = [\"Interphase\", \"Prometaphase\", \"Metaphase\", \"Anaphase\", \"Apoptosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Testing Dataset from the zip files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels, states = read_annotations(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the \"load_model\" function from the CellX library, we can import models without needing to specify the CellX custom layers that had been used to build them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model'\n",
    "model = load_model(f'{model_name}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the images in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image normalization function\n",
    "def normalize_image_array(img):\n",
    "    img_mean = np.mean(img)\n",
    "    img_stddev = max(np.std(img), 1.0/np.size(img))\n",
    "    img = np.subtract(img,img_mean)\n",
    "    img = np.divide(img,img_stddev)\n",
    "    # clip to 4 standard deviations\n",
    "    img = np.clip(img, -4, 4)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [normalize_image_array(image) for image in test_images]\n",
    "test_images_array = np.array(test_images)[...,np.newaxis] # convert to numpy array for model prediction\n",
    "test_labels_array = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Model on the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_images_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'softmax' function transforms test_predictions into an array of scores for each class for each instance in the testing set. Across classes, the scores sum to one. The class associated with the highest score is the model's 'prediction'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = softmax(test_predictions,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on Testing Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample N images out of the testing set to check the model's predictions on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_testing_predictions(\n",
    "    num_examples, # number of testing examples to show\n",
    "    test_images\n",
    "):\n",
    "    plt.figure(figsize=(10,3*(int(num_examples/5)+1)))\n",
    "    plt.suptitle('Predictions',fontsize=25,x=0.5,y=0.95)\n",
    "    for image_num in range(min(np.shape(test_images_array)[0],num_examples)-1):\n",
    "        plt.subplot(int(num_examples/5)+1,5,image_num+1)\n",
    "        plt.imshow(test_images_array[image_num,:,:,0])\n",
    "        plt.title('Image {}'.format(image_num+1))\n",
    "        plt.yticks([])\n",
    "        plt.xticks([])\n",
    "        plt.xlabel(LABELS[np.argmax(test_predictions[image_num])])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_testing_predictions(20,test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next calculate the \"precision\", \"recall\" and \"F1 score\" metrics for each class, as well as the \"confusion matrix\" for the CNN's performance on the testing set. The three metrics are calculated using the number of \"false positive\", \"true positive\" and \"false negative\" predictions for each class.\n",
    "- The \"precision\" of class X is calculated by $$precision(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;positives}$$\n",
    "- The \"recall\" of class X is calculated by $$recall(X) = \\frac{No.\\;of\\;true\\;positives}{No.\\;of\\;true\\;positives+No.\\;of\\;false\\;negatives}$$\n",
    "- The \"F1 score\" of class X is calculated by $$F1(X) = 2*\\frac{precision(X)*recall(X)}{precision(X)+recall(X)}$$\n",
    "<br>\n",
    "\n",
    "The \"confusion matrix\" is a table that visually represents the performance of a network on a testing set. The number shown in row A and column B is the number of testing examples of ground-truth class A that have been predicted as belonging to class B by the network.\n",
    "\n",
    "Reading resource for confusion matrices: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(test_images_array, test_labels_array)\n",
    "\n",
    "test_confusion_matrix = confusion_matrix(test_labels,np.argmax(test_predictions,axis=1))\n",
    "test_confusion_matrix_plot = plot_confusion_matrix(test_confusion_matrix,LABELS)\n",
    "test_confusion_matrix_plot.show()\n",
    "\n",
    "print('Testing Accuracy = ',accuracy)\n",
    "print('Testing Loss = ',loss)\n",
    "\n",
    "precision,recall,fscore,support = precision_recall_fscore_support(test_labels,np.argmax(test_predictions,axis=1))\n",
    "print('Testing Precision = ',precision)\n",
    "print('Testing Recall = ',recall)\n",
    "print('Testing F1 Score = ',fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction with UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running the below cell, we see that the model output is an array of 2 dimensions: \n",
    "* the 1st dimension corresponds to the number of test images used \n",
    "* the 2nd dimension corresponds to the number of possible classes predefined in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use UMAP to easily visualise the network's classification performance by embedding the predictions from 5D space (number of classes/features) into a lower 2D space while attempting to keep the data's inherent structure and underlying relationships.\n",
    "\n",
    "We first define our parameters of choice. In this simple example, we chose to only modify the following ones:\n",
    "* `n_neighbors` - the number of neighbours determines the size of the local neighbourhood that UMAP should focus on when creating the embedding, low values => emphasis on local structure, high values => emphasis on global structure\n",
    "* `n_epochs` - the number of epochs determines the number of rounds the UMAP embedding will be optimised for (similar to training a CNN), the higher the number the more accurately the 2D embedding will replicate the original data structure\n",
    "* `random_state` - UMAP is a stochastic algorithm, so we need to set a random seed to ensure that the results are reproducible across different runs. try eliminating this parameter, you should see slightly different UMAP embeddings from one run to the next\n",
    "\n",
    "Feel free to adjust the parameters and check how the below image projection changes! You can read up on the most important parameters [here](https://umap-learn.readthedocs.io/en/latest/parameters.html#) or go through the whole list of parameters [here](https://umap-learn.readthedocs.io/en/latest/api.html).\n",
    "\n",
    "If you're interested in reading about how UMAP works, [see here](https://umap-learn.readthedocs.io/en/latest/basic_usage.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP parameters\n",
    "nbs = 5\n",
    "eps = 50\n",
    "rnd = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a UMAP model with the defined parameters. The full configuration of the UMAP model will be printed out with all the parameter values to be used, including the ones modified above. \n",
    "\n",
    "Note:`verbose=True`enables written feedback to the user while UMAP is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = UMAP(n_neighbors=nbs, n_epochs=eps, random_state=rnd, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the UMAP model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper.fit(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image patch projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting the test images corresponding to the test predictions on top of the UMAP embedding, we can visually assess whether single-cell patches of the same class correctly cluster together in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert single-channel test images to rgb three-channel images\n",
    "print(f\"shape of test images: {test_images_array.shape}\")\n",
    "rgb_images = np.concatenate([test_images]*3, axis=-1)\n",
    "print(f\"shape of rgb test images: {rgb_images.shape}\")\n",
    "# normalise image values to 0-1 range (Min-Max scaling) & convert to 8-bit\n",
    "rgb_images = ((rgb_images-np.min(rgb_images))/(np.ptp(rgb_images)) * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the grid of image patches corresponding to the UMAP embedding. (This is basically a 2D histogram where points on a same grid cell are binned and the average of the binned images is projected.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projection = ManifoldProjection2D(rgb_images)\n",
    "img_grid, heatmap, delimiters = projection(mapper.embedding_, components=(0,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure to show the image projection. The last line allows you to save the projection to the Colab \"content\" folder, but remember to then go on the \"...\" button next to the file generated in the Files tab (click the Refresh button if you don't see it) in order to download a local copy of the file. Reminder: Files saved during a Colab session will be lost upon closing this session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "\n",
    "im = plt.imshow(img_grid,\n",
    "                origin=\"lower\",\n",
    "#                 extent=delimiters, \n",
    "                cmap=\"gray\",)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.colorbar()\n",
    "\n",
    "# (optional) uncomment the below line to save the UMAP image patch projection\n",
    "# fig.savefig(f\"umap_{mapper.n_neighbors}nbs_rnd{mapper.random_state}.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
