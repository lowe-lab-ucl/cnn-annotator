{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Image Annotation with Napari\n",
    "\n",
    "### Welcome! \n",
    "\n",
    "This notebook allows you to import your raw microscopy data into [_Napari_](https://napari.org/ \"Napari: a fast, interactive, multi-dimensional image viewer for Python\"), a fast, interactive, multi-dimensional image viewer for Python. Follow the step-wise instructions to annotate your large, multi-dimensional images and extract single-cell image patches corresponding to different states of the cell cycle. \n",
    "\n",
    "This is a preview of the *napari* interface for image annotations:\n",
    "![image](../napari_annotator.png)\n",
    "\n",
    "\n",
    "\n",
    "### Running Instructions:\n",
    "\n",
    "1. To run this notebook, go to ```Kernel``` and click on ```Restart & Run All```. Alternatively, for a shortcut, click on the ```‚è≠Ô∏è ``` in the top dashboard. It's the icon right next to the ```Run ‚ñ∂Ô∏è```, ``` ‚èπÔ∏è```, ``` üîÅ``` and the ```Code``` dropdown window. This will restart the notebook & run all of its cells.\n",
    "\n",
    "2. Calling the last cell will start a separate Python window with the *napari* widget. Please allow a few seconds to load the graphical user interface (GUI) to load. You should see your movie loaded on the screen, such as here:\n",
    "\n",
    "\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- Make sure you have the latest version of *napari* installed in your virtual environment. If in doubt, run ```pip install napari -U``` for installing the upgrade. \n",
    "- Do not forget to export your annotation zip file before terminating the session. The annotations will not be autosaved nor exported automatically, so failing to export your annotations will result in your work being lost!\n",
    "\n",
    "---\n",
    "\n",
    "## Annotating the Single-Cell Image Patches:\n",
    "\n",
    "To train a neural network to automatically and accurately classify previously unseen single-cell images, we need to manually annotate a handful of such image patches with a label. This is because the learning approach we use to train such a network is *supervised* (by a human annotator) - this means that for the learning algorithm to master the patterns representative for each class, we need to provide some of the images with the correct answer, often referred to as the *'ground truth'*. This allows the network to make high-fidelity predictions about previously unseen images.\n",
    "\n",
    "In this section, a random sample of labeled images needs to be provided by annotating any microscopy movie of choice. The rule of thumb is that the more examples you can generate the better. We recommend to label at least **100 instances per each class** for reasonably accurate training. Using the guide below, please annotate each image with one of the five labels provided.\n",
    "\n",
    "Here are some examples of cells and their corresponding labels:\n",
    "![image](../cell_cycle_states.png)\n",
    "\n",
    "\n",
    "### Annotation Instructions:\n",
    "\n",
    "1. Annotate your movie by clicking on the individual instances of the labels you wish to annotate. Although there is no sub-pixels targetting accuracy required at this stage, please aim to click at the centre of each cell / nucleus to allow the image patches to be cropped around those coordinates properly. The default labels is *interphase*.\n",
    "\n",
    "2. Change the labels for which you wish to annotate at the bottom left corner of the GUI by choosing the appropriate label from the dropdown menu. There should be 5 labels available: *interphase (default), prometaphase, metaphase, anaphase and apoptosis*. You can swiftly change between the class labels by pressing ```.``` or ```,``` keyboard key for the next or previous label, respectively.\n",
    "\n",
    "3. When done annotating one image, you can move to the next image by clicking on the ```right arrow``` or ```left arrow``` keyboard key. If this doesn't change the slide in the current view, click on the progress bar / track slider to activate this feature. \n",
    "\n",
    "4. In case of mis-clicking on a cell you wish not to include in the annotations, to erase a point from the annotations list, choose the ```Delete point``` button at the top left corner of the GUI. You can also choose a ```Multi-point selection``` and delete the unwanted points in bulk. \n",
    "\n",
    "5. When done annotating, export the annotation file by clicking the ```Export``` button at the very bottom right corner of the GUI. **Do not forget to export your annotations before terminating the session. The annotations will not be autosaved nor exported automatically.** \n",
    "\n",
    "6. *Have you exported the annotations?* Close the GUI by clicking on the red ```X``` to quit the python GUI completely. *Optional:* If using OS X, choose to 'Force quit' the napari window for completeness. Doing so will cause this notebook's kernel to die. That's perfectly fine.\n",
    "\n",
    "---\n",
    "\n",
    "**Happy annotating!**\n",
    "\n",
    "*Your [CellX](http://lowe.cs.ucl.ac.uk/cellx.html \"Lowe Lab @ UCL\") team*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import some useful libraries:\n",
    "\n",
    "We first need to load some libraries of code that will help with the data processing and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import json\n",
    "import napari\n",
    "import numpy as np\n",
    "\n",
    "from enum import Enum\n",
    "from magicgui import magicgui\n",
    "from skimage.io import imread\n",
    "from pathlib import Path\n",
    "\n",
    "from napari.layers import Image\n",
    "from datetime import datetime\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from skimage.external.tifffile import TiffWriter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide the directory for the movie you wish to annotate:\n",
    "\n",
    "In this example, we provide an example 50-frame long crop (600 x 450 pixels) of a time-lapse microscopy movie ```MDCK_H2B_GFP_movie.tif``` of MDCK cells expressing an *H2B-GFP* fluorescent tag, which visualises the nuclei of the individual cells. This movie has dimensions (*i.e.* shape) of ```(50, 450, 600)```. When loading your own movie, make sure it is saved as a ```tif``` file and provide its absolute path.\n",
    "\n",
    "Unit conversion: *1 ¬µm = 3 pixels.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../MDCK_H2B_GFP_movie.tif\n"
     ]
    }
   ],
   "source": [
    "filename = \"../MDCK_H2B_GFP_movie.tif\"\n",
    "print (filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the image data from the movie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = imread(filename)\n",
    "metadata = {\"filename\": filename}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the cell cycle states which you'd like your CNN to categorise:\n",
    "\n",
    "Create an object with enumerated classes which you want to classify. In this particular example, we provide 4 classes of actively dividing cells (*i.e. **interphase** pooled for G1-, S- and G2-phases & **pro(meta)phase, metaphase & ana(telo)phase** for specific phases of cell division*) and 1 class for ceasing cells (*i.e. **apoptosis***). \n",
    "\n",
    "To familiarise yourself with the typical cell morphologies & chromatin condensation of an actively dividing cell, please see the example images below:\n",
    "\n",
    "![image](../cell_cycle_states.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellState(Enum):\n",
    "    Interphase = 0\n",
    "    Prometaphase = 1\n",
    "    Metaphase = 2\n",
    "    Anaphase = 3\n",
    "    Apoptosis = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign the colours to individual labelled states:\n",
    "\n",
    "The default settings will follow the ```matplotlib``` standard colour library with {\"blue\", \"orange\", \"green\", \"red\", \"purple\"}. \n",
    "\n",
    "*Note:* When changing the default setting or defining new colour palettes, please specify the [HEX code](https://www.color-hex.com/ \"HEX Color Codes\") for new colours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_CYCLE = [\n",
    "    '#1f77b4', # blue\n",
    "    '#ff7f0e', # orange\n",
    "    '#2ca02c', # green\n",
    "    '#d62728', # red\n",
    "    '#9467bd', # purple\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function crops an image patch around the labelled points:\n",
    "\n",
    "Default setting will crop a 64 x 64 pixel square image patch with the labelled point at the centre of the patch, i.e. 32 pixels up, 32 pixels down, 32 pixels left & 32 pixels right from the labelled coordinates. Please only alter with care.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_patch(layers, coords, shape=64):\n",
    "    \"\"\" Get an image patch from the image layer data. \"\"\"\n",
    "    \n",
    "    frame, y_coo, x_coo = [int(coo) for coo in coords]\n",
    "    pad = shape // 2\n",
    "    \n",
    "    # Read the image & crop patch around the point:\n",
    "    image = layers[0].data[frame]\n",
    "    patch = np.array([row[x_coo - pad : x_coo + pad] for row in image[y_coo - pad : y_coo + pad]]).astype(np.uint8)\n",
    "    \n",
    "    # Check if patches are of specified shape:\n",
    "    if patch.shape != (shape, shape):\n",
    "        raise ValueError(f\"Image patch doesn't have correct shape: {patch.shape}\")\n",
    "    \n",
    "    return patch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The annotator function:\n",
    "\n",
    "For those users more experienced at Python, you can read the code the below & possibly add more ```key_bindings``` functions for an even smoother annotation in *napari*: visit this [guide](https://napari.org/docs/0.3.8/_modules/napari/utils/key_bindings.html \"Source code for napari.utils.key_bindings\"). Otherwise, no manipulation is encouraged at this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotator(viewer):\n",
    "    \n",
    "    SESSION_TIME = datetime.now().strftime(\"%m-%d-%Y--%H-%M-%S\")\n",
    "    SESSION_NAME = f\"annotation_{SESSION_TIME}\"\n",
    "    \n",
    "    # add an empty points layer, with the same dimensions as the image data\n",
    "    points_layer = viewer.add_points(\n",
    "        name=\"Annotation\", \n",
    "        properties={'State': [s.name for s in CellState]}, \n",
    "        ndim=data.ndim\n",
    "    )\n",
    "\n",
    "    points_layer.mode = 'add'\n",
    "    points_layer.face_color = 'State'\n",
    "    points_layer.face_color_cycle = COLOR_CYCLE\n",
    "    points_layer.face_color_mode = 'cycle'\n",
    "    # points_layer.n_dimensional = True\n",
    "    \n",
    "    @magicgui(\n",
    "        call_button=\"Export\",\n",
    "        layout=\"horizontal\",\n",
    "        filename={\"label\": \"Export path:\"},  # custom label\n",
    "    )\n",
    "    \n",
    "    def cnn_annotation_widget(\n",
    "        filename=Path.home(),  # path objects are provided a file picker\n",
    "        shape=64,\n",
    "        use_visible_layers=True,\n",
    "        state=CellState.Interphase,\n",
    "    ):\n",
    "        \"\"\" Export the annotations: \"\"\"\n",
    "        \n",
    "        export_data = {'shape': shape}\n",
    "        \n",
    "        # find the visible image layers and export the metadata\n",
    "        image_layers = [layer for layer in viewer.layers if isinstance(layer, Image)]\n",
    "        for layer in image_layers:\n",
    "            if use_visible_layers and layer.visible:\n",
    "                export_data[layer.name] = layer.metadata\n",
    "        \n",
    "        # record the coordinates of the annotations \n",
    "        for idx in range(points_layer.data.shape[1]):\n",
    "            export_data[f'coords-{idx}'] = points_layer.data[:, idx].tolist()\n",
    "        \n",
    "        # record the state labels of the annotations \n",
    "        export_data['labels'] = points_layer.properties['State'].tolist()\n",
    "        \n",
    "        # extract the image patches here\n",
    "        with ZipFile(f\"{SESSION_NAME}.zip\", 'w') as myzip:\n",
    "            for idx, patch_coords in enumerate(points_layer.data):\n",
    "                \n",
    "                patch_label = points_layer.properties['State'][idx]\n",
    "\n",
    "                # grab the image patch\n",
    "                image_patch = get_image_patch(image_layers, patch_coords, shape=shape)\n",
    "                image_patch_fn = f\"{patch_label}/{patch_label}_{SESSION_TIME}_{idx}.tif\"\n",
    "                \n",
    "                # open a stream to write to the zip file\n",
    "                stream = io.BytesIO()\n",
    "                with TiffWriter(stream) as tif:\n",
    "                    tif.save(image_patch)\n",
    "                    stream_data = stream.getvalue()\n",
    "                myzip.writestr(image_patch_fn, stream_data)\n",
    "        \n",
    "            # write out the json log to the zip file also\n",
    "            stream = json.dumps(export_data, indent=2)\n",
    "            myzip.writestr(f\"{SESSION_NAME}.json\", stream)\n",
    "        \n",
    "        print (f\"JSON file & image patches have been exported.\\n'{SESSION_NAME}.zip'\")\n",
    "        \n",
    "        return locals().values()\n",
    "    \n",
    "    def _change_points_properties(event):\n",
    "        \"\"\" Update the current properties of the points layer to reflect the currently selected state. \"\"\"\n",
    "        points_layer.current_properties['State'] = np.array([cnn_annotation_widget.state.value.name])\n",
    "    \n",
    "    cnn_annotation_widget.state.changed.connect(_change_points_properties)\n",
    "    \n",
    "    # add the magicgui dock widget \n",
    "    viewer.window.add_dock_widget(cnn_annotation_widget)\n",
    "    \n",
    "    @viewer.bind_key('.')\n",
    "    def next_label(event=None):\n",
    "        \"\"\" Increment the label in the GUI \"\"\"\n",
    "        new_state = (cnn_annotation_widget.state.value.value + 1) % len(CellState)\n",
    "        cnn_annotation_widget.state.value = CellState(new_state)\n",
    "        \n",
    "    @viewer.bind_key(',')\n",
    "    def previous_label(event=None):\n",
    "        \"\"\" Decrement the label in the GUI \"\"\"\n",
    "        new_state = (cnn_annotation_widget.state.value.value - 1) % len(CellState)\n",
    "        cnn_annotation_widget.state.value = CellState(new_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the cell below will open *napari* in a separate Python window. \n",
    "\n",
    "*Note:* ***Please allow a few seconds for the GUI to load.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with napari.gui_qt():\n",
    "    \n",
    "    viewer = napari.Viewer()\n",
    "    viewer.add_image(data, name='GFP', metadata=metadata)\n",
    "   \n",
    "    annotator(viewer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the annotations:\n",
    "\n",
    "1. Click on the ```Export``` button at the bottom right corner of the *napari* window when done annotating.\n",
    "2. The name of the exported file will be printed under the cell above. Please check it was saved successfully in the folder.\n",
    "3. Close the *napari* window & quit the python GUI completely.\n",
    "4. Doing so will cause this notebook's kernel to die. That's perfectly fine.\n",
    "\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "1. If you prefer to check the quality of your image patches and visualise some of your overall data statistics, please visit the **B_CNN_Data_Preview_Images.ipynb** iPython notebook\n",
    "2. If you'd like to proceed directly to the neural network training step in the Google Colab environment, please visit the **C_CNN_Training_and_Validation.ipynb** iPython notebook\n",
    "\n",
    "\n",
    "#### Done! You can close this notebook now."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
